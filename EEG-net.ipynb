{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your CSV files\n",
    "data_dir = 'C:\\\\Users\\\\artur\\\\Desktop\\\\Studia\\\\Projekt inzynierski\\\\data'\n",
    "\n",
    "# Read CSV files and stack data\n",
    "def load_eeg_data(data_dir):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            data = pd.read_csv(filepath)\n",
    "            all_data.append(data.values)\n",
    "    \n",
    "    # Stack all data samples into a single array\n",
    "    data_array = np.stack(all_data, axis=0)  # Shape: (N, T, C)\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (36, 30999, 19)\n"
     ]
    }
   ],
   "source": [
    "data_array = load_eeg_data(data_dir)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Data shape: {data_array.shape}\")\n",
    "\n",
    "# Normalize the data\n",
    "data_array = data_array.astype(np.float32)\n",
    "data_array = (data_array - np.min(data_array)) / (np.max(data_array) - np.min(data_array))  # Normalize to [0, 1]\n",
    "\n",
    "# Ensure the data is in the format (N, C, T, 1)\n",
    "N, T, C = data_array.shape\n",
    "data = data_array.transpose(0, 2, 1)[:, :, :, np.newaxis]  # Shape: (N, C, T, 1)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        return x, x  # In autoencoder, target is the same as input\n",
    "\n",
    "# Create datasets and dataloaders for training and testing\n",
    "train_dataset = EEGDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = EEGDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # No shuffle for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=(1, 5), padding=(0, 2)),  # Output: (N, 16, T, C)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=(1, 5), padding=(0, 2)),  # Output: (N, 8, T, C)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=(1, 5), padding=(0, 2)),  # Output: (N, 16, T, C)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, in_channels, kernel_size=(1, 5), padding=(0, 2)),  # Output: (N, in_channels, T, C)\n",
    "            nn.Sigmoid()  # Sigmoid to ensure output is in range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = data.shape[1]\n",
    "model = SimpleAutoencoder(in_channels=in_channels)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 2/2 [00:02<00:00,  1.08s/batch, loss=0.0578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Average Loss: 0.05789574049413204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 2/2 [00:02<00:00,  1.03s/batch, loss=0.0575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Average Loss: 0.057579297572374344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 2/2 [00:01<00:00,  1.02batch/s, loss=0.0572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Average Loss: 0.05725984647870064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 2/2 [00:02<00:00,  1.03s/batch, loss=0.0569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Average Loss: 0.05693719908595085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 2/2 [00:01<00:00,  1.03batch/s, loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Average Loss: 0.05660444684326649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 2/2 [00:02<00:00,  1.02s/batch, loss=0.0562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Average Loss: 0.056268395856022835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 2/2 [00:02<00:00,  1.01s/batch, loss=0.0559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Average Loss: 0.05592348240315914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 2/2 [00:02<00:00,  1.01s/batch, loss=0.0555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Average Loss: 0.05556577444076538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 2/2 [00:02<00:00,  1.03s/batch, loss=0.0551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Average Loss: 0.055197011679410934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 2/2 [00:02<00:00,  1.01s/batch, loss=0.0547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Average Loss: 0.05481266602873802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 2/2 [00:02<00:00,  1.01s/batch, loss=0.0543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Average Loss: 0.05441712774336338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 2/2 [00:02<00:00,  1.01s/batch, loss=0.0539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Average Loss: 0.054003043100237846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 2/2 [00:02<00:00,  1.01s/batch, loss=0.0534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Average Loss: 0.053564876317977905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 2/2 [00:01<00:00,  1.01batch/s, loss=0.053] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Average Loss: 0.053102822974324226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 2/2 [00:02<00:00,  1.03s/batch, loss=0.0525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Average Loss: 0.0526100005954504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 2/2 [00:02<00:00,  1.01s/batch, loss=0.0519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Average Loss: 0.052089499309659004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 2/2 [00:02<00:00,  1.05s/batch, loss=0.0514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Average Loss: 0.051534635946154594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 2/2 [00:02<00:00,  1.04s/batch, loss=0.0508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Average Loss: 0.050935545936226845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 2/2 [00:02<00:00,  1.00s/batch, loss=0.0501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Average Loss: 0.05029614642262459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 2/2 [00:02<00:00,  1.00s/batch, loss=0.0494]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Average Loss: 0.04960709437727928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    for x_batch, _ in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, x_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss/len(train_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for anomalies: 0.0490\n",
      "Number of anomalies detected: 1\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a trained model and a test DataLoader\n",
    "def calculate_reconstruction_error(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, _ in dataloader:  # No need for labels\n",
    "            reconstructed = model(inputs)  # Get reconstructed output\n",
    "            \n",
    "            # Calculate the reconstruction error (Mean Squared Error)\n",
    "            mse = torch.mean((inputs - reconstructed) ** 2, dim=[1, 2, 3])  # Mean over all dimensions\n",
    "            reconstruction_errors.extend(mse.cpu().numpy())  # Store errors\n",
    "\n",
    "    return np.array(reconstruction_errors)\n",
    "\n",
    "# Call the function and get reconstruction errors\n",
    "reconstruction_errors = calculate_reconstruction_error(model, test_dataloader)\n",
    "\n",
    "# Set a threshold for anomaly detection (this can be tuned)\n",
    "threshold = np.percentile(reconstruction_errors, 95)  # E.g., 95th percentile\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = reconstruction_errors > threshold\n",
    "\n",
    "# Print some information\n",
    "print(f\"Threshold for anomalies: {threshold:.4f}\")\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
